version: "2"

services:
  # HDFS :
  namenode:
    image: hadoop-namenode
#    hostname: namenode
    container_name: namenode
#    domainname: hadoop
    ports:
      - "50070:50070"  # 外网ip口
      - "8020:8020"    # 节点端口
    volumes:
      - ~/hdata/namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=test
      - NAMENODE_ADDR=0.0.0.0
    env_file:
      - ./hadoop.env

  datanode-1:
    image: hadoop-datanode
#    hostname: datanode-1
    container_name: datanode1
#    domainname: hadoop
#    extra_hosts:  # 生产环境 云端配置
#      - namenode:0.0.0.0
    depends_on:
      - namenode
    links:
      - "namenode:namenode"
    ports:
        - "50071:50071"  # 外网ip口
        - "50011:50011"
        - "50021:50021"
    volumes:
      - ~/hdata/datanode1:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:50011
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:50071
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:50021
    env_file:
      - ./hadoop.env

  datanode-2:
    image: hadoop-datanode
#    hostname: datanode-2
    container_name: datanode2
#    domainname: hadoop
#    extra_hosts:  # 生产环境 云端配置
#      - namenode:0.0.0.0
    depends_on:
      - namenode
    links:
      - "namenode:namenode"
    ports:
      - "50072:50072"   # 外网ip口
      - "50012:50012"
      - "50022:50022"
    volumes:
      - ~/hdata/datanode2:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_datanode_http_address=0.0.0.0:50072
      - HDFS_CONF_dfs_datanode_address=0.0.0.0:50012
      - HDFS_CONF_dfs_datanode_ipc_address=0.0.0.0:50022
    env_file:
      - ./hadoop.env

#  SecondaryNameNode:
#    image: hadoop-secondmanager
##    hostname: secondmanager
#    container_name: secondmanager
##    domainname: hadoop
##    net: hadoop
#    depends_on:
#      - namenode
#    links:
#      - "namenode:namenode"
#    ports:
#      - "50090：50090" # 外网web端口
#    environment:
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
#    env_file:
#      - ./hadoop.env

  # YARN :
#  resourcemanager:
#    image: hadoop-resourcemanager
#    hostname: resourcemanager
#    container_name: resourcemanager
#    domainname: hadoop
##    net: hadoop
#    ports:
#    - "8088:8088"
#    - "8032:8032"
#    env_file:
#      - ./hadoop.env
#
#
#  nodemanager-1:
#    image: hadoop-nodemanager
#    hostname: nodemanager1
#    container_name: nodemanager1
#    domainname: hadoop
##    net: hadoop
#    ports:
#      - "8042:8042"
#    env_file:
#    - ./hadoop.env
#
#  historyserver:
#    image: hadoop-historyserver
#    hostname: historyserver
#    container_name: historyserver
#    domainname: hadoop
##    net: hadoop
#    volumes:
#      - ./data/historyserver:/hadoop/yarn/timeline
#    env_file:
#      - ./hadoop.env
#    ports:
#      - "8188:8188"

#   spark (支持yarn-cluster/yarn-client/standalone):
#  spark:
#    image: hadoop-spark
##    hostname: spark
#    container_name: spark
##    domainname: hadoop
#    environment:
#      - SPARK_CONF_spark_eventLog_enabled=true
#      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:9000/spark-logs
#      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:9000/spark-logs
#    env_file:
#      - ./hadoop.env
#    ports:
#      - "4040:4040"

#  spark-master:
#    image: hadoop-spark-master
##    hostname: spark-master
#    container_name: spark-master
#    environment:
#      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
#      - SPARK_CONF_spark_eventLog_enabled=true
#      - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
#      - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
#    depends_on:
#      - namenode
#    links:
#      - "namenode:namenode"
#    env_file:
#    - ./hadoop.env
#    ports:
#      - "8080:8080"
#      - "7077:7077"
#    volumes:
#    - ~/PycharmProjects/TensorFlowOnSpark:/opt/TensorFlowOnSpark
#
#  spark-worker-1:
#    image: hadoop-spark-worker
##    hostname: spark-worker-1
#    container_name: spark-worker-1
#    environment:
#    - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#    - SPARK_CONF_spark_eventLog_enabled=true
#    - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:8020/spark-logs
#    - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
#    - SPARK_MASTER_URL=spark://spark-master:7077
#    - SPARK_WORKER_PORT=7071
#    - SPARK_WORKER_WEBUI_PORT=8081
#    depends_on:
#      - spark-master
#    links:
#      - "spark-master:spark-master"
#      - "namenode:namenode"
#    ports:
#      - "7071:7071"
#      - "8081:8081"
#    env_file:
#    - ./hadoop.env
#
#  spark-worker-2:
#    image: hadoop-spark-worker
##    hostname: spark-worker-2
#    container_name: spark-worker-2
#    environment:
#    - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
#    - SPARK_CONF_spark_eventLog_enabled=true
#    - SPARK_CONF_spark_eventLog_dir=hdfs://namenode:9000/spark-logs
#    - SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:8020/spark-logs
#    - SPARK_MASTER_URL=spark://spark-master:7077
#    - SPARK_WORKER_PORT=7072
#    - SPARK_WORKER_WEBUI_PORT=8082
#    depends_on:
#      - spark-master
#    links:
#      - "spark-master:spark-master"
#      - "namenode:namenode"
#    ports:
#      - "7072:7072"
#      - "8082:8082"
#    env_file:
#    - ./hadoop.env

  #spark-notebook:
  #  image: hadoop-spark-notebook
  #  hostname: sparknotebook
  #  container_name: sparknotebook
  #  domainname: hadoop
  #  net: hadoop
  #  environment:
  #    - SPARK_NOTEBOOK_MASTER=yarn-client
  #  env_file:
  #    - ./hadoop.env



